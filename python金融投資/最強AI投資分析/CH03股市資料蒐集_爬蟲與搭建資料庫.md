# CH03è‚¡å¸‚è³‡æ–™è’é›†_çˆ¬èŸ²èˆ‡æ­å»ºè³‡æ–™åº«

## 3-2 è³‡æ–™çˆ¬èŸ²

## è­‰äº¤æ‰€è³‡æ–™çˆ¬èŸ²
- 1.é€²å…¥è­‰äº¤æ‰€ç¶²å€ï¼šhttps://www.twse.com.tw/zh/index.html
- 2.ä½¿ç”¨é–‹ç™¼è€…æ¨¡å¼å–å¾—è«‹æ±‚è³‡æ–™ç¶²å€

```python
### 1ï¸âƒ£ åŒ¯å…¥å¥—ä»¶

import requests
import pandas as pd
import datetime as dt # æ™‚é–“å¥—ä»¶
from dateutil.relativedelta import relativedelta

### 2ï¸âƒ£ å–å¾—å€‹è‚¡æ—¥æˆäº¤è³‡è¨Š"""

# è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ
stock_id = '2330'
# ç•¶æ—¥æ™‚é–“
date = dt.date.today().strftime("%Y%m%d")
# å–å¾—è­‰äº¤æ‰€ç¶²ç«™è³‡æ–™
stock_data = requests.get(f'https://www.twse.com.tw/rwd/zh/ \
            afterTrading/STOCK_DAY?date={date}&stockNo={stock_id}')
json_data = stock_data.json()
df = pd.DataFrame(data=json_data['data'],
                  columns=json_data['fields'])
df.tail()

### 3ï¸âƒ£ å–å¾—é€£çºŒæœˆä»½è³‡æ–™(ä»¥å€‹è‚¡æœ¬ç›Šæ¯”ç‚ºä¾‹)

# è¨­å®šæŠ“å–å¹¾å€‹æœˆè³‡æ–™
month_num=3
date_now = dt.datetime.now()

# å»ºç«‹æ—¥æœŸä¸²åˆ—
date_list = [(date_now - relativedelta(months=i)).replace(day=1).\
             strftime('%Y%m%d') for i in range(month_num)]

date_list.reverse()
all_df = pd.DataFrame()

# ä½¿ç”¨è¿´åœˆæŠ“å–é€£çºŒæœˆä»½è³‡æ–™
for date in date_list:
  url = f'https://www.twse.com.tw/rwd/zh/afterTrading/\
      BWIBBU?date={date}&stockNo={stock_id}'
  try:
    json_data = requests.get(url).json()
    df = pd.DataFrame(data=json_data['data'],
                  columns=json_data['fields'])
    all_df = pd.concat([all_df, df], ignore_index=True)
  except Exception as e:
    print(f"ç„¡æ³•å–å¾—{date}çš„è³‡æ–™, å¯èƒ½è³‡æ–™é‡ä¸è¶³.")

all_df.head()
```
## ç”¨ BeautifulSoup4 å–å¾— Yahoo è‚¡å¸‚è³‡æ–™
```python

###4ï¸âƒ£ åŒ¯å…¥å¥—ä»¶
from datetime import datetime
from bs4 import BeautifulSoup
import time

###  5ï¸âƒ£ å–å¾—ç•¶æ—¥è‚¡åƒ¹

def yahoo_stock(stock_id):
    url = f'https://tw.stock.yahoo.com/quote/{stock_id}.TW'
    # ä½¿ç”¨ requests å–å¾—ç¶²é å…§å®¹
    response = requests.get(url)
    html = response.content
    # ä½¿ç”¨ Beautiful Soup è§£æž HTML å…§å®¹
    soup = BeautifulSoup(html, 'html.parser')
    # ä½¿ç”¨ find èˆ‡ find_all å®šä½å…ƒç´ 
    time_element = soup.find('section',\
                {'id': 'qsp-overview-realtime-info'}).find('time')
    table_soups = soup.find('section',\
                {'id': 'qsp-overview-realtime-info'}).find('ul')\
                                   .find_all('li')
    fields = []
    datas = []
    for table_soup in table_soups:
        table_datas = table_soup.find_all('span')
        for num,table_data in enumerate(table_datas):
            if table_data.text =='':
                continue
            if num == 0:
                fields.append(table_data.text)
            else:
                datas.append(table_data.text)
    # å»ºç«‹ DataFrame
    df = pd.DataFrame([datas], columns=fields)
    # å¢žåŠ æ—¥æœŸå’Œè‚¡è™Ÿæ¬„ä½
    df.insert(0,'æ—¥æœŸ',time_element['datatime'])
    df.insert(1,'è‚¡è™Ÿ',stock_id)
    # å›žå‚³ DataFrame
    return df

yahoo_stock(stock_id)

### 6ï¸âƒ£ å–å¾—å­£å ±è¡¨è³‡è¨Š
# å‡½å¼å¯ç”¨æ–¼å¥‡æ‘©è²¡å ±
def url_find(url):
    words = url.split('/')
    k = words[-1]
    # ä½¿ç”¨requestså–å¾—ç¶²é å…§å®¹
    response = requests.get(url)
    html = response.content
    # ä½¿ç”¨Beautiful Soupè§£æžHTMLå…§å®¹
    soup = BeautifulSoup(html, 'html.parser')
    # æ‰¾åˆ°è¡¨æ ¼çš„è¡¨é ­
    table_soup = soup.find('section', {'id': 'qsp-{}-table'.format(k)})
    table_fields=table_soup.find('div', class_='table-header')
    table_fields_lines = list(table_fields.stripped_strings)
    # æ‰¾åˆ°æ•¸æ“š
    data_rows = table_soup.find_all('li' ,class_='List(n)')
    # è§£æžè³‡æ–™è¡Œå…§å®¹
    data = []
    for row in data_rows:
        row_data = list(row.stripped_strings)
        data.append(row_data)
    # å»ºç«‹ DataFrame
    df = pd.DataFrame(data, columns=table_fields_lines)
    return df

# æŠ“æç›Šè¡¨
url = f'https://tw.stock.yahoo.com/quote/{stock_id}/income-statement'

# æŠ“è³‡ç”¢è² å‚µè¡¨
# url = f'https://tw.stock.yahoo.com/quote/{stock_id}/balance-sheet'

# æŠ“ç¾é‡‘æµé‡è¡¨
# url = f'https://tw.stock.yahoo.com/quote/{stock_id}/cash-flow-statement'

# æŠ“å–å­£å ±è¡¨è³‡æ–™
df = url_find(url).transpose()

# è³‡æ–™è™•ç†
df.columns = df.iloc[0]
df = df[1:]
df.insert(0,'å¹´åº¦/å­£åˆ¥',df.index)
df.columns.name = None
df.reset_index(drop=True, inplace=True)

df.tail()
```
## ä½¿ç”¨ selenium åšæ–°èžçˆ¬èŸ²
```python

### 7ï¸âƒ£ ç”¨ requests å–å¾—è‚¡ç¥¨æ–°èž

# è‚¡ç¥¨ä»£è™Ÿ
stock_id = "2330"
# é è¨­è¡¨æ ¼æ•¸æ“šå’Œæ¬„ä½
field=['è‚¡è™Ÿ','æ—¥æœŸ','æ¨™é¡Œ','å…§å®¹']
data=[]
# å–å¾— Json æ ¼å¼è³‡æ–™
json_data = requests.get(f'https://ess.api.cnyes.com/ess/api/'
                f'v1/news/keyword?q={stock_id}&limit=20&page=1').json()
# ä¾ç…§æ ¼å¼æ“·å–è³‡æ–™
items=json_data['data']['items']
for item in items:
    # ç¶²å€ã€æ¨™é¡Œå’Œæ—¥æœŸ
    news_id = item["newsId"]
    title = item["title"]
    publish_at = item["publishAt"]
    # ä½¿ç”¨ UTC æ™‚é–“æ ¼å¼
    utc_time = datetime.utcfromtimestamp(publish_at)
    formatted_date = utc_time.strftime('%Y-%m-%d')
    # å‰å¾€ç¶²å€æ“·å–å…§å®¹
    url = requests.get(f'https://news.cnyes.com/'
                       f'news/id/{news_id}').content
    soup = BeautifulSoup(url, 'html.parser')
    p_elements = soup.find_all('p')
    # æå–æ®µè½å†…å®¹
    p=''
    for paragraph in p_elements[4:]:
        p+=paragraph.get_text()
    data.append([stock_id, formatted_date ,title,p])
# å»ºç«‹è¡¨æ ¼
df = pd.DataFrame(data,columns=field)
df
```
### 8ï¸âƒ£  å®‰è£åŠåŒ¯å…¥å¥—ä»¶"""
```python
!pip install selenium
from selenium import webdriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')  # ä¸é¡¯ç¤ºç€è¦½å™¨
chrome_options.add_argument('--no-sandbox')# ä»¥æœ€é«˜æ¬Šé™é‹è¡Œ

### 9ï¸âƒ£ ä½¿ç”¨ Selenium å–å¾—è‚¡ç¥¨æ–°èž

# é€éŽ options è¨­å®š driver
driver = webdriver.Chrome(options=chrome_options)
data2=[] # è¡¨æ ¼æ•¸æ“š

# ç›®æ¨™ç¶²å€
url = f"https://www.cnyes.com/search/news?keyword={stock_id}"
driver.get(url)

# æ¨¡æ“¬æ»‘å‹•æ»‘é¼ æ»¾è¼ªçš„è¡Œç‚ºï¼Œç”¨æ–¼åŠ è¼‰æ›´å¤šå…§å®¹
scroll_pause_time = 2  # ç­‰å¾…æ™‚é–“
last_height = driver.execute_script(
                    "return document.body.scrollHeight")
while True:
    driver.execute_script(
        "window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(scroll_pause_time)
    new_height = driver.execute_script(
        "return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

elements = driver.find_elements("xpath",
                        '//*[@id="_SearchAll"]/section/div/a')

# æ“·å–ç¶²å€å’Œæ¨™é¡Œ
for element in elements:
    link = element.get_attribute("href")
    title = element.text
    title=title.split('\n')
    data2.append([stock_id, title[1] ,title[0],link])

# é—œé–‰ç€è¦½å™¨
driver.quit()
for link in data2:
  # ä½¿ç”¨ requests å‰å¾€ç¶²å€æ“·å–æ–°èžå…§å®¹
  link_a = requests.get(link[3]).content
  link_b = BeautifulSoup(link_a,'html.parser')
  p_elements = link_b.find('article')
  # å–å¾—æ®µè½å†…å®¹
  try:
    link[3] = p_elements.get_text()
  except:
    link[3] = 'ç„¡å…§å®¹'
# å»ºç«‹è¡¨æ ¼
df = pd.DataFrame(data2,columns=field)
df.tail()

# æŠŠè³‡æ–™å­˜æˆ csv å¯ä»¥ç”¨ä»¥ä¸‹é€™æ®µ
df.to_csv('/content/news_data.csv' )
```
# 3.3 ç”¨ Python å¥—ä»¶è¼•é¬†å–å¾—è‚¡å¸‚è³‡æ–™

## ä½¿ç”¨ yfinance ä¸‹è¼‰è‚¡å¸‚è³‡æ–™
```python

### ðŸ”Ÿ  å®‰è£åŠåŒ¯å…¥å¥—ä»¶

!pip install yfinance==0.2.38

import yfinance as yf

### 1ï¸âƒ£1ï¸âƒ£  è¨­å®šè‚¡ç¥¨ä»£ç¢¼å’Œèµ·æ­¢æ™‚é–“

# æŒ‡å®šè¦ä¸‹è¼‰çš„è‚¡ç¥¨ä»£ç¢¼, ä¸Šå¸‚ç‚º .TW;ä¸Šæ«ƒç‚º .TWO
stock_id = '2330.TW'

# è¨­å®šé–‹å§‹èˆ‡çµæŸæ™‚é–“
end = dt.date.today()
start = end - dt.timedelta(days=360)

### 1ï¸âƒ£2ï¸âƒ£ å–å¾—æ¯æ—¥è‚¡åƒ¹ (é–‹é«˜ä½Žæ”¶) è³‡æ–™
stock_data = yf.download(stock_id, start=start, end=end)
stock_data.tail()

# ä¾ç…§è³‡æ–™æœŸé–“ä¸‹è¼‰
stock_data = yf.download(stock_id, period="3mo")

# ä¸‹è¼‰ä¸åŒæ™‚é–“é »çŽ‡çš„è³‡æ–™ (1åˆ†K)
stock_data = yf.download(stock_id, interval="1m")

### 1ï¸âƒ£3ï¸âƒ£ å–å¾—å¤šæª”è‚¡ç¥¨çš„è³‡æ–™

stocks = [stock_id, '2303.TW', '2454.TW'] #åˆ†åˆ¥ç‚ºå°ç©é›»ã€è¯é›»å’Œè¯ç™¼ç§‘
stock_data = yf.download(stocks, start=start, end=end)
stock_data.tail()

### 1ï¸âƒ£4ï¸âƒ£ å–å¾—å…¬å¸åŸºæœ¬è³‡æ–™


stock = yf.Ticker(stock_id)
stock.info # ç‚ºå­—å…¸åž‹æ…‹

### 1ï¸âƒ£5ï¸âƒ£  å–å¾—æç›Šè¡¨"""

financials = stock.financials
financials.tail()

bs = stock.balance_sheet
bs

### 1ï¸âƒ£6ï¸âƒ£ å–å¾—æ³•äººæŒè‚¡æ¯”ä¾‹
#### å›  yfinance è³‡æ–™å•é¡Œï¼Œæ­¤å„²å­˜æ ¼æš«æ™‚ç„¡æ³•ä½¿ç”¨
institutional_holders = stock.institutional_holders
institutional_holders.tail()
```
## ä½¿ç”¨ FinMind ä¸‹è¼‰è‚¡å¸‚è³‡æ–™
```python
### 1ï¸âƒ£7ï¸âƒ£  å®‰è£åŠåŒ¯å…¥å¥—ä»¶

!pip install FinMind

from FinMind.data import DataLoader
import getpass

### 1ï¸âƒ£8ï¸âƒ£  è¼¸å…¥ FinMind API å’Œå¸³è™Ÿå¯†ç¢¼

token = getpass.getpass("è«‹è¼¸å…¥ FinMind é‡‘é‘°ï¼š")

### 1ï¸âƒ£9ï¸âƒ£  å»ºç«‹ FinMind è³‡æ–™åº«ç‰©ä»¶å’Œç™»å…¥ FinMind

api = DataLoader()
api.login_by_token(api_token=token)

### 2ï¸âƒ£0ï¸âƒ£ å–å¾—è‚¡åƒ¹è³‡æ–™

# è¨­å®šè‚¡ç¥¨ä»£è™Ÿ
stock_id = '2330'

# è³‡æ–™æœŸé–“
end = dt.date.today()
start = end - dt.timedelta(days=360)

stock_data =  api.taiwan_stock_daily(
    stock_id=stock_id,
    start_date=start,
    end_date=end)

stock_data.tail()

### 2ï¸âƒ£1ï¸âƒ£ å–å¾—æç›Šè¡¨è³‡æ–™

financial_data = api.taiwan_stock_financial_statement(
    stock_id=stock_id,
    start_date=str(start),)

financial_data.tail()

### 2ï¸âƒ£2ï¸âƒ£ å–å¾—æ³•äººè²·è³£è³‡æ–™

investors_data = api.taiwan_stock_institutional_investors(
    stock_id=stock_id,
    start_date=str(start),)
investors_data.tail()
```
## ä½¿ç”¨ FinLab ä¸‹è¼‰è‚¡å¸‚è³‡æ–™
- æ³¨æ„ï¼FinLab ç›®å‰éœ€ä»˜è²»æ‰èƒ½å–å¾—æœ€æ–°è³‡æ–™
```python

### 2ï¸âƒ£3ï¸âƒ£ å®‰è£åŠåŒ¯å…¥å¥—ä»¶
!pip install finlab

import finlab
from finlab import data

### 2ï¸âƒ£4ï¸âƒ£ ç™»å…¥ FinLab

token = getpass.getpass("è«‹è¼¸å…¥FinLabé‡‘é‘°ï¼š")
finlab.login(token)

### 2ï¸âƒ£5ï¸âƒ£ å–å¾—æ”¶ç›¤åƒ¹

close = data.get('price:æ”¶ç›¤åƒ¹')
close.tail()

### 2ï¸âƒ£6ï¸âƒ£ é¸æ“‡ç”¢æ¥­

data.set_universe(market='TSE', category='åŠå°Žé«”')
close = data.get('price:æ”¶ç›¤åƒ¹')
close.tail()

### 2ï¸âƒ£7ï¸âƒ£ å–å¾—è²¡å ±è³‡æ–™

df = data.get('financial_statement:æ¯è‚¡ç›ˆé¤˜')
df.tail()

### 2ï¸âƒ£8ï¸âƒ£  å–å¾—æ³•äººè³‡æ–™

df = data.get('institutional_investors_trading_summary:æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸')
df.tail()
```
# 3.4 SQLè³‡æ–™åº«
```python
### 2ï¸âƒ£9ï¸âƒ£ åŒ¯å…¥å¥—ä»¶åŠé€£ç·šè³‡æ–™åº«

import sqlite3
conn = sqlite3.connect('stock.db')

### 3ï¸âƒ£0ï¸âƒ£è¨­å®šè³‡æ–™åº«çµæ§‹

cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS æ—¥é »è³‡æ–™ (
    sno INTEGER PRIMARY KEY AUTOINCREMENT,
    Stock_Id TEXT,
    Date DATE,
    Open FLOAT,
    High FLOAT,
    Low FLOAT,
    Close FLOAT,
    Adj_Close FLOAT,
    Volume INTEGER
);
''')
conn.commit()

### 3ï¸âƒ£1ï¸âƒ£ å‚³å…¥è³‡æ–™åˆ°è³‡æ–™åº«

df = yf.download('2330.TW',start='2023-08-01')
df = df.reset_index()
df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
df.rename(columns={"Adj Close": "Adj_Close"}, inplace=True)
df.insert(0,'Stock_id','2330')

df.to_sql('æ—¥é »è³‡æ–™',conn,if_exists='append',index=False)

### 3ï¸âƒ£2ï¸âƒ£ æŸ¥è©¢è¡¨æ ¼è³‡æ–™

def table_info(table_name):
    cursor = conn.cursor()
    cursor.execute(f"PRAGMA table_info({table_name})")
    columns = cursor.fetchall()
    column_names = [column[1] for column in columns]
    print(f"è³‡æ–™åº«è¡¨ '{table_name}' çš„æ¬„ä½åç¨±ï¼š", column_names)
    all_data = conn.execute(f'SELECT * FROM {table_name}')
    for i in all_data.fetchall():
        print(i)

# æŸ¥è©¢è¡¨æ ¼è³‡æ–™
table_info("æ—¥é »è³‡æ–™")

### 3ï¸âƒ£3ï¸âƒ£ æ–°å¢žè³‡æ–™

def insert_data(stock_id, start):
  # ä¸‹è¼‰è³‡æ–™
  df = yf.download(f'{stock_id}.TW',start=start)
  df = df.reset_index()
  df.rename(columns={"Adj Close": "Adj_Close"}, inplace=True)
  df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
  df.insert(0,'Stock_Id',stock_id)

  # æ–°å¢žè³‡æ–™è¡¨
  df.to_sql('æ—¥é »è³‡æ–™',conn,if_exists='append',index=False)

# æ–°å¢ž 2317 è³‡æ–™
insert_data(stock_id=2317, start='2023-08-01')

### 3ï¸âƒ£4ï¸âƒ£ å¾žè³‡æ–™åº«å–å¾—è³‡æ–™ ==> è³‡æ–™èª²æŸ¥è©¢

query = ("SELECT Stock_id, Date, Close "
         "FROM æ—¥é »è³‡æ–™ "
         "WHERE Date < '2023-08-15' AND Stock_id = '2317'")
df = pd.read_sql(query, conn, parse_dates=['Date'])
df.tail()

### 3ï¸âƒ£5ï¸âƒ£ ä¿®æ”¹ä¸¦é—œé–‰è³‡æ–™åº«

conn.commit()
conn.close()
```
