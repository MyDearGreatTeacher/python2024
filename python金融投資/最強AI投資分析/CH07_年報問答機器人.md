
# CH-07 å¹´å ±å•ç­”æ©Ÿå™¨äºº

## 7-2 å–å¾—å¹´å ±

```python

### 1ï¸âƒ£  åŒ¯å…¥å¥—ä»¶

import requests
from bs4 import BeautifulSoup

### 2ï¸âƒ£ å»ºç«‹å‡½å¼-å–å¾—å¹´å ±

def annual_report(id,y):
  url = 'https://doc.twse.com.tw/server-java/t57sb01'
  # å»ºç«‹ POST è«‹æ±‚çš„è¡¨å–®
  data = {
      "id":"",
      "key":"",
      "step":"1",
      "co_id":id,
      "year":y,
      "seamon":"",
      "mtype":'F',
      "dtype":'F04'
  }
  try:
    # ç™¼é€ POST è«‹æ±‚
    response = requests.post(url, data=data)
    # å–å¾—å›æ‡‰å¾Œæ“·å–æª”æ¡ˆåç¨±
    link=BeautifulSoup(response.text, 'html.parser')
    link1=link.find('a').text
    print(link1)
  except Exception as e:
    print(f"ç™¼ç”Ÿ{e}éŒ¯èª¤")
  # å»ºç«‹ç¬¬äºŒå€‹ POST è«‹æ±‚çš„è¡¨å–®
  data2 = {
      'step':'9',
      'kind':'F',
      'co_id':id,
      'filename':link1 # æª”æ¡ˆåç¨±
  }
  try:
    # ç™¼é€ POST è«‹æ±‚
    response = requests.post(url, data=data2)
    link=BeautifulSoup(response.text, 'html.parser')
    link1=link.find('a')
    # å–å¾— PDF é€£çµ
    link2 = link1.get('href')
    print(link2)
  except Exception as e:
    print(f"ç™¼ç”Ÿ{e}éŒ¯èª¤")
  # ç™¼é€ GET è«‹æ±‚
  try:
    response = requests.get('https://doc.twse.com.tw' + link2)
    # å–å¾— PDF è³‡æ–™
    with open(y + '_' + id + '.pdf', 'wb') as file:
        file.write(response.content)
    print('OK')
  except Exception as e:
    print(f"ç™¼ç”Ÿ{e}éŒ¯èª¤")

### 3ï¸âƒ£ å‘¼å«å‡½å¼

annual_report('2330','113')
```
## 7-3 å¹´å ±å•ç­”
```python
###4ï¸âƒ£ å®‰è£ç›¸é—œå¥—ä»¶
"""
ç”±æ–¼ç‰ˆæœ¬æ›´æ–°çš„ç›¸å®¹æ€§å•é¡Œ, pdfplumberä½¿ç”¨èˆŠç‰ˆæœ¬çš„0.10.2
"""

!pip install langchain==0.0.335 openai tiktoken pdfplumber==0.10.2 faiss-cpu

###  5ï¸âƒ£ åŒ¯å…¥ç›¸é—œå¥—ä»¶

import os
import getpass
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI

### 6ï¸âƒ£ è¨­å®šç’°å¢ƒè®Šæ•¸å’Œå»ºç«‹ OpenAI æ¨¡å‹

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
llm_16k = ChatOpenAI(model="gpt-3.5-turbo")

### 7ï¸âƒ£ å»ºç«‹å‡½å¼-å»ºç«‹å‘é‡è³‡æ–™åº«

def pdf_loader(file,size,overlap):
  loader = PDFPlumberLoader(file)
  doc = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(
                          chunk_size=size,
                          chunk_overlap=overlap)
  new_doc = text_splitter.split_documents(doc)
  db = FAISS.from_documents(new_doc, OpenAIEmbeddings())
  return db

### 8ï¸âƒ£  å‘¼å«å‡½å¼

db = pdf_loader('/content/113_2330.pdf',500,50)

### 9ï¸âƒ£ æŸ¥è©¢ç›¸é—œè³‡æ–™

query = "å…¬å¸æ˜¯å¦æœ‰æ˜ç¢ºçš„æˆé•·æˆ–å‰µæ–°ç­–ç•¥?"
docs = db.similarity_search(query,k=3)
for i in docs:
    print(i.page_content)
    print('_________')

### ğŸ”Ÿ  åŒ¯å…¥å•ç­”ç›¸é—œå¥—ä»¶

from langchain.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA

### 1ï¸âƒ£1ï¸âƒ£  å»ºç«‹å‡½å¼-å•ç­”ç¨‹å¼

# æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "ä½ æ˜¯ä¸€å€‹æ ¹æ“šå¹´å ±è³‡æ–™èˆ‡ä¸Šä¸‹æ–‡ä½œå›ç­”çš„åŠ©æ‰‹,"
     "å¦‚æœæœ‰æ˜ç¢ºæ•¸æ“šæˆ–æŠ€è¡“(ç”¢å“)åç¨±å¯ä»¥ç”¨æ•¸æ“šæˆ–åç¨±å›ç­”,"
     "å›ç­”ä»¥ç¹é«”ä¸­æ–‡å’Œå°ç£ç”¨èªç‚ºä¸»ã€‚"
     "{context}"),
    ("human","{question}")])

# å»ºç«‹å•ç­”å‡½å¼
def question_and_answer(question):
    qa = RetrievalQA.from_llm(llm=llm_16k,
                              prompt=prompt,
                              return_source_documents=True,
                              retriever=db.as_retriever(
                                  search_kwargs={'k':10}))
    result = qa(question)
    return result

### 1ï¸âƒ£2ï¸âƒ£ å»ºç«‹è¿´åœˆé€²è¡Œå•ç­”

while True:
    question = input("è¼¸å…¥å•é¡Œ:")
    if not question.strip():
        break
    result=question_and_answer(question)
    print(result['result'])
    print('_________')
    #print(result["source_documents"])
```
## 7-4 å¹´å ±ç¸½çµèˆ‡åˆ†æ
```python
### 1ï¸âƒ£3ï¸âƒ£ å›ç­”çµæœåŠåŸå§‹è³‡æ–™

from langchain.chains.summarize import load_summarize_chain

### 1ï¸âƒ£4ï¸âƒ£ ç¸½çµåŸå§‹è³‡æ–™

# å»ºç«‹é—œéµå­—ä¸²åˆ—
key_word = ['æœ‰é—œå¸‚å ´ç­–ç•¥çš„èª¿æ•´æˆ–è®ŠåŒ–æœ‰ä½•æåŠï¼Ÿ',
          'å…¬å¸å°æœªä¾†ä¸€å¹´çš„å±•æœ›æ˜¯ä»€éº¼ï¼Ÿ',
          'å…¬å¸çš„ç¸½æ”¶å…¥æ˜¯å¦å¢é•·ï¼Œæ·¨åˆ©æ½¤çš„æ­£è² æƒ…æ³æ˜¯å¦æœ‰è®ŠåŒ–ï¼Ÿ',
          'åœ‹éš›ç«¶çˆ­åŠæµ·å¤–å¸‚å ´æƒ…æ³',
          'ç›®å‰çš„ç ”ç™¼ç‹€æ³?']
data_list = []
for word in key_word:
    data = db.max_marginal_relevance_search(word)
    # æ•´åˆ Document ä¸²åˆ—
    data_list += data

# å»ºç«‹æç¤ºè¨Šæ¯ä¸²åˆ—
prompt_template = [("system","ä½ çš„ä»»å‹™æ˜¯ç”Ÿæˆå¹´å ±æ‘˜è¦ã€‚"
                "æˆ‘å€‘æä¾›å¹´å ±{text}è«‹ä½ è² è²¬ç”Ÿæˆ,"
                "ä¸”è¦ä¿ç•™é‡é»å¦‚ç‡Ÿæ”¶æ¼²è·Œã€é–‹ç™¼é …ç›®ç­‰,"
                "æœ€å¾Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡è¼¸å‡ºå ±å‘Š")]
prompt = ChatPromptTemplate.from_messages(messages=prompt_template)

### 1ï¸âƒ£5ï¸âƒ£  å‘¼å«å‡½å¼

chain_refine_16k = load_summarize_chain(llm=llm_16k,
                                        chain_type='stuff',
                                        prompt=prompt)
print(chain_refine_16k.run(data_list))

### 1ï¸âƒ£6ï¸âƒ£  æå–é—œéµå­—

from langchain.chains import LLMChain
from langchain.output_parsers import CommaSeparatedListOutputParser
output_parser = CommaSeparatedListOutputParser()

word_prompt=ChatPromptTemplate.from_messages(messages=[
    ("human","å¾{input}è¯æƒ³å‡º4å€‹èˆ‡å¹´å ±åˆ†ææœ‰é—œçš„é‡è¦é—œéµå­—,"\
     "è«‹ç¢ºä¿å›ç­”å…·æœ‰å…·æœ‰é—œè¯æ€§ã€å¤šæ¨£æ€§å’Œè®ŠåŒ–æ€§ã€‚ \n "
     "åƒ…å›è¦†é—œéµå­—, ä¸¦ä»¥åŠå½¢é€—è™Ÿèˆ‡ç©ºæ ¼ä¾†åˆ†éš”ã€‚ä¸è¦åŠ å…¥å…¶ä»–å…§å®¹"
    "")]
)

word_chain = LLMChain(llm=llm_16k, prompt=word_prompt)
output_parser.parse(word_chain('å…¬å¸çš„ç‡Ÿæ”¶ç‹€æ³å¦‚ä½•ï¼Ÿ')['text'])

### 1ï¸âƒ£7ï¸âƒ£ è¨­å®š AI è§’è‰²è®“å…¶åˆ†æå ±å‘Š

data_prompt=ChatPromptTemplate.from_messages(messages=[
    ("system","ä½ ç¾åœ¨æ˜¯ä¸€ä½å°ˆæ¥­çš„è‚¡ç¥¨åˆ†æå¸«,"
    "ä½ æœƒä»¥è©³ç´°ã€åš´è¬¹çš„è§’åº¦é€²è¡Œå¹´å ±åˆ†æ, é‡å°{output}ä½œåˆ†æä¸¦æåŠé‡è¦æ•¸å­—\
    ,ç„¶å¾Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„è¶¨å‹¢åˆ†æå ±å‘Šã€‚"),
    ("human","{text}")])
data_chain = LLMChain(llm=llm_16k, prompt=data_prompt)

### 1ï¸âƒ£8ï¸âƒ£ æ•´åˆå‡½å¼

def analyze_chain(input):
    # æœå°‹ã€Œå•é¡Œã€çš„ç›¸é—œè³‡æ–™
    data = db.max_marginal_relevance_search(input)

    # ç¬¬ä¸€å€‹ Chain å…ƒä»¶, å»ºç«‹ã€Œé—œéµå­—ã€ä¸²åˆ—
    word = word_chain(input)
    word_list = output_parser.parse(word['text'])

    # æœå°‹ã€Œé—œéµå­—ã€çš„ç›¸é—œè³‡æ–™
    for i in word_list:
      data += db.max_marginal_relevance_search(i,k=2)
    word_list.append(input)

    # ç¬¬äºŒå€‹ Chain å…ƒä»¶, ç”Ÿæˆåˆ†æå ±å‘Š
    result = data_chain({'output':word_list,'text':data})

    return result['text']

### 1ï¸âƒ£9ï¸âƒ£ å‘¼å«å‡½å¼

input = 'å…¬å¸çš„ç‡Ÿæ”¶ç‹€æ³å¦‚ä½•ï¼Ÿ'
print(analyze_chain(input))
```
